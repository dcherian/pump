#+title: PUMP
#+hugo_base_dir: ./hugo/
#+hugo_section:
#+options: :eval never-export
#+property: header-args:jupyter-python :session /jpy:localhost#8800:emacs

* To do                                                            :noexport:
- [ ] Do velocity spectra (only at equator?)
- [ ] spectra vs TAO vs SST
- [ ] filtered hovmoellerr for SST
- [ ] What are TAO locations with most data?
- [ ] freq of shred > 0?
- [ ] surface stress, net heat flux, N^2 profile
- [ ] plot differences in mean state between solutions
- [ ] Update script for heat budget runs
- [ ] composite DCL / TIW

- [ ] composite like Inoue et al (2019).

- What do profiles look like with daily data?

* startup                                                          :noexport:

#+NAME: startup
#+BEGIN_SRC jupyter-python :results none :exports none
%matplotlib inline

import dask
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import seawater as sw
import xarray as xr

# import hvplot.xarray

import dcpy
import pump

# import facetgrid

mpl.rcParams['savefig.dpi'] = 300
mpl.rcParams['savefig.bbox'] = 'tight'
mpl.rcParams['figure.dpi'] = 250

xr.set_options(keep_attrs=True)
#+END_SRC

#+NAME: build-cluster
#+BEGIN_SRC jupyter-python :results none
import dask
import distributed
import ncar_jobqueue

if 'client' in locals():
    client.close(); cluster.close()
cluster = ncar_jobqueue.NCARCluster(
    cores=1, processes=1, memory='25GB',
    walltime='02:00:00', project='UMIT0018')

client = dask.distributed.Client(cluster)

# cluster, client = pump.utils.build_cluster()
#+END_SRC

#+NAME: scale-cluster
#+BEGIN_SRC jupyter-python :var n=2 :results output drawer
cluster.scale(n)
#+END_SRC
#+RESULTS: scale-cluster
:results:
<Client: scheduler='tcp://10.12.205.27:42720' processes=6 cores=12>
:end:


* read model runs :noexport:

#+NAME: read-gcm1-hb
#+BEGIN_SRC jupyter-python
gcm1 = pump.model('../glade/TPOS_MITgcm_1_hb/HOLD/',
                  name='gcm1', full=True, budget=False)
#+END_SRC

#+RESULTS: read-gcm1-hb
#+begin_example
/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.
  coords=coords)
/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want
to use the new `combine_by_coords` function (or the
`combine='by_coords'` option to `open_mfdataset` to order the datasets
before concatenation. Alternatively, to continue concatenating based
on the order the datasets are supplied in in future, please use the
new `combine_nested` function (or the `combine='nested'` option to
open_mfdataset).
  coords=coords)
Reading all files took 53.04539155960083 seconds
/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.
  coords=coords)
/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want
to use the new `combine_by_coords` function (or the
`combine='by_coords'` option to `open_mfdataset` to order the datasets
before concatenation. Alternatively, to continue concatenating based
on the order the datasets are supplied in in future, please use the
new `combine_nested` function (or the `combine='nested'` option to
open_mfdataset).The datasets supplied require both concatenation and merging. From
xarray version 0.14 this will operation will require either using the
new `combine_nested` function (or the `combine='nested'` option to
open_mfdataset), with a nested list structure such that you can combine
along the dimensions None. Alternatively if your datasets have global
dimension coordinates then you can use the new `combine_by_coords`
function.
  coords=coords)
#+end_example

#+NAME: read-gcm1
#+BEGIN_SRC jupyter-python
gcm1 = pump.model('../glade/TPOS_MITgcm_1/HOLD/',
                  name='gcm1', full=False, budget=False)
#+END_SRC

#+RESULTS: read-gcm1
#+begin_example
metrics files not available.
/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py:783: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated.
  coords=coords)
/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py:783: FutureWarning: The datasets supplied have global dimension coordinates. You may want
to use the new `combine_by_coords` function (or the
`combine='by_coords'` option to `open_mfdataset` to order the datasets
before concatenation. Alternatively, to continue concatenating based
on the order the datasets are supplied in in future, please use the
new `combine_nested` function (or the `combine='nested'` option to
open_mfdataset).
  coords=coords)
#+end_example

* gyaan

** Marginal instability

1. In marginally unstable systems, Ri tends to cluster around 1/4 citep:Smyth2019 : this is /scale invariance/ --- This is why the usual averaging bias in Ri estimates is not a problem at the equator citep:Pham2017. This behaviour is seen for stratified turbulence forced by a mean shear that varies slowly on the time scale of the instabilities.

2. cite:Smyth2013a grid T, ADCP on 5m grid and then differentiate. Use N2 = g Î± T_z.

** TIWs
- Meridional velocity seems to be a good way to define phases.

Inoue papers:
- N-S phase has strongest mixing
- S phase -> UCL (-300 W/m^2; 1e-3 m^2/s)
  - EQUIX was the first expt where microstructure measurements were taken during the S phase
  - Can models recover this?
- Turbulence appears to be proportional to Sh^2_{red}
- S-N, N phases have weakest mixing / heat flux + strongest dT/dz
- S^2 , N^2 co-vary (Inoue et al, 2012; Figure 8)

Ryan:
- Zonal shear is the main thing. Meridional shear is not so important.
-


* Data locations                                                   :noexport:

|--------+-------------------|
| ROMS   | glade/tpos20/OUT/ |
| MITgcm | glade/TPOS_MITgcm  |
| POP    | g.xxx             |
|--------+-------------------|

- heat budget output are 4 hourly snapshots
- others are daily averages

* Vertical resolution

#+NAME: 541ec1bc-e56b-4910-8b49-ad9476538313
#+BEGIN_SRC jupyter-python :session localhost:8888/pump/notebooks/validation.ipynb :results output drawer
%time gcm0 = pump.model('../glade/TPOS_MITgcm/HOLD/', 'gcm20 orig')
%time gcm1 = pump.model('../glade/TPOS_MITgcm_1/HOLD/', 'gcm20 1m')
%time gcm25 = pump.model('../glade/TPOS_MITgcm_2.5/HOLD/', 'gcm20 2.5m')
%time gcm5 = pump.model('../glade/TPOS_MITgcm_5/HOLD/', 'gcm20 5m')
%time gcm10 = pump.model('../glade/TPOS_MITgcm_10/HOLD/', 'gcm20 10m')

models = dict(zip(['gcm', '1m', '2.5m', '5m', '10m'],
                  [gcm0, gcm1, gcm25, gcm5, gcm10]))
#+END_SRC

#+RESULTS: 541ec1bc-e56b-4910-8b49-ad9476538313
:results:
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
<timed exec> in <module>

~/pump/pump/model/model.py in __init__(self, dirname, name, kind, full)
     50
     51         self.obs = obs_container()
---> 52         self.oisst = read_sst(self.domain['xyt'])
     53
     54         try:

~/pump/pump/obs.py in read_sst(domain)
     97         sst = xr.open_mfdataset(
     98             [root+'/obs/oisst/sst.day.mean.'+str(yy)+'.nc' for yy in years],
---> 99             parallel=True)
    100     else:
    101         sst = xr.open_mfdataset(root+'/obs/oisst/sst.day.mean.*.nc',

/gpfs/u/home/dcherian/python/xarray/xarray/backends/api.py in open_mfdataset(paths, chunks, concat_dim, compat, preprocess, engine, lock, data_vars, coords, autoclose, parallel, **kwargs)
    704         # calling compute here will return the datasets/file_objs lists,
    705         # the underlying datasets will still be stored as dask arrays
--> 706         datasets, file_objs = dask.compute(datasets, file_objs)
    707
    708     # Close datasets in case of a ValueError

~/miniconda3/envs/dcpy/lib/python3.6/site-packages/dask/base.py in compute(*args, **kwargs)
    396     keys = [x.__dask_keys__() for x in collections]
    397     postcomputes = [x.__dask_postcompute__() for x in collections]
--> 398     results = schedule(dsk, keys, **kwargs)
    399     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])
    400

~/miniconda3/envs/dcpy/lib/python3.6/site-packages/distributed/client.py in get(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)
   2330             try:
   2331                 results = self.gather(packed, asynchronous=asynchronous,
-> 2332                                       direct=direct)
   2333             finally:
   2334                 for f in futures.values():

~/miniconda3/envs/dcpy/lib/python3.6/site-packages/distributed/client.py in gather(self, futures, errors, maxsize, direct, asynchronous)
   1654             return self.sync(self._gather, futures, errors=errors,
   1655                              direct=direct, local_worker=local_worker,
-> 1656                              asynchronous=asynchronous)
   1657
   1658     @gen.coroutine

~/miniconda3/envs/dcpy/lib/python3.6/site-packages/distributed/client.py in sync(self, func, *args, **kwargs)
    674             return future
    675         else:
--> 676             return sync(self.loop, func, *args, **kwargs)
    677
    678     def __repr__(self):

~/miniconda3/envs/dcpy/lib/python3.6/site-packages/distributed/utils.py in sync(loop, func, *args, **kwargs)
    278     else:
    279         while not e.is_set():
--> 280             e.wait(10)
    281     if error[0]:
    282         six.reraise(*error[0])

~/miniconda3/envs/dcpy/lib/python3.6/threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--> 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553

~/miniconda3/envs/dcpy/lib/python3.6/threading.py in wait(self, timeout)
    297             else:
    298                 if timeout > 0:
--> 299                     gotit = waiter.acquire(True, timeout)
    300                 else:
    301                     gotit = waiter.acquire(False)

KeyboardInterrupt:
:end:


* Validations
:PROPERTIES:
:EXPORT_FILE_NAME: validations
:EXPORT_HUGO_SECTION: validations
:END:

** Todo
- [X] Barotropic flow and slope of EUC

- [ ] Diurnal cycle

- [X] meridional profile of the EUC. How wide is it? What depth is the maximum width? This can be compared with the Johnson et al data at different longitudes.

- [ ] Meridional shear and location/strength of NECC

- [X] shear above the EUC. Are the two lobes of the westward SEC above it realistic? (Again Johnson, also the long equatorial mooring time series at 110W, 140W, 170W, 165E). The site with these is shut down now but will presumably be up again soon.

- [ ] Variability of TIWs (amplitude, lateral extent, frequency)

- [ ] the TIWs you mention will be a good test, since Frank has shown that his 0.1-degree run has much stronger TIWs than the 1-degree run. Do they get even stronger at 1/20th? Does the cold tongue front sharpen? Beyond the present project, we need to understand the role of model resolution on TIWs, since they are so fundamental to the upper heat budget. There is limited data to test this (a few short-term experiments), except SST may be useful.

- [ ] temporal variation of U, V, shear, stratification within TIW cycle in mixed layer, pycnocline, EUC core, and below at 110, 125, and 140W. Especially, compare  with observations that we have found in 2008.

** Summary

1. EUC is narrower and weaker than Johnson climatology. EUC maximum is slightly deeper (20m or so).
2.

** Turbulence

|------------------------------+---------------------------------------+-------------------------------+----------------------------|
| Diagnostic / Model           | observations                          | 1m                            | 10m                        |
|------------------------------+---------------------------------------+-------------------------------+----------------------------|
| Marginal stability at 0, 140 | Ri = 0.25,                            | Ri=0.3-0.4                    | Ri=0.3-0.4                 |
|                              | MAM: more stable (0.5-1)              | MAM: not different! (0.4-0.5) | MAM: more stable (0.5-0.6) |
|------------------------------+---------------------------------------+-------------------------------+----------------------------|
| Deep cycle layer (DCL)       | Daily cycle; seasonal cycle in depth  |                               |                            |
|------------------------------+---------------------------------------+-------------------------------+----------------------------|
| Upper core layer (UCL)       | 20 - 40m thick layer above EUC max    |                               |                            |
|                              | decoupled from DCL / surface at times |                               |                            |
|------------------------------+---------------------------------------+-------------------------------+----------------------------|
| TIW modulation               | Largest during N, N-S phases          |                               |                            |
| (not independent)             |                                       |                               |                            |
|------------------------------+---------------------------------------+-------------------------------+----------------------------|

*** Marginal (in)stability

#+CAPTION: Seasonal median Ri profiles like cite:Smyth2013a for TAO locations along the equator. This uses daily average output.
[[file:images/seasonal-Ri-tao.png]]

#+CAPTION: Compare gcm runs to TAO at (0, 140). Still biased high. The TAO estimates are with daily-averaged output.
[[file:images/Ri-all-models.png]]

*** Deep cycle

Definitions:
1. Depth of max squared shear
2. dÎµ/dt : since DCL is a daily cycle in Îµ. Average Îµto 6h intervals and then compute dÎµ/dt. Identify penetration of daily varying epsilon + choose greatest depth of penetration

**** Zaron & Moum

KPP defines /mixing layer/ as Ri < 0.3 which would include the deep cycle layer?

But Zaron & Moum show SBL (i.e. mixing layer depth) as being different from MLD and deep cycle?

Large & Gent: Pacanowski & Philander scheme has much higher diffusivities because they need that to get a surface mixed layer but KPP has a surface layer mixing scheme to take care of that.

**** Are the models simulating a deep cycle?
 a. 1m:

 b. 10m: Hmmm..
  #+CAPTION: Not sure if the 10m simulation actually has a deep cycle. The descending shear max  corresponds to base of the mixed layer. (c) DCL $K_T$ mean, median (d,e) Solid lines are MLD, DCL base, EUC max.
  [[file:images/maybe-dcl-10m.png]]

** SST

#+CAPTION: 1996 Monthly mean SST from OISST and MITgcm.
[[file:images/monthly-mean-sst.png]]

** Surface velocity

#+CAPTION: Monthly mean sea-surface zonal velocity. OSCAR vs MITgcm
[[file:images/monthly-mean-ssu.png]]

** EUC

#+CAPTION: Meriodional sections of the EUC in the Johnson climatology (black) and MITgcm 1/20 (gray). First 3 columns: Meriodional profile is averaged -250m to surface. 4th column: vertical profile is averaged between -3N to 3N, -250m to surface, for u > 0.
[[file:images/mitgcm-20-johnson-depth-sections.png]]


#+CAPTION: Depth-longitude sections for MITgcm 1/20 vs Johnson climatology. Slope looks good! Model EUC is slightly deeper.
[[file:images/mitgcm-20-johnson-longitude-depth-section.png]]
** NECC
** Spectra
#+CAPTION: Multitaper spectra for 100m temperature. TAO vs MITgcm 1/20.
[[file:images/validation-mitgcm20-tao-100m-temp-spectra.png]]

** TIW

#+CAPTION: Hovmoeller plots of SST anomaly from OISST (color) & MITgcm (black)
[[file:images/oisst-comparison.png]]


* Diary

** <2019-06-10 Mon>

- No luck so far with a new DCL base definition
- There seems to be large variation for each TIW "period" though composites at 110W, 125W, 140W are consistent

** <2019-05-14 Tue>

- Looking for deep cycle signal. I may or may not see it. Hard to be sure.
  [[file:images/maybe-dcl-10m.png]]

* Meetings
:PROPERTIES:
:EXPORT_FILE_NAME: meetings
:EXPORT_HUGO_SECTION: meetings
:END:

** <2019-03-20 Wed>

*** Results

- Simulation domain begins at 95W. Do we move this further east to avoid edge effects?

*** Comments
- [ ] Do vertical profile of transport instead of mean velocity.
*** Followup

* TAO
* Marginal stability

** groupby_bins
#+BEGIN_SRC jupyter-python :session py
da = xr.DataArray([[0,1],[2,3]],
                  {'lon': (['ny','nx'], [[30,40],[40,50]] ),
                   'lat': (['ny','nx'], [[10,10],[20,20]] ),},
                  dims=['ny','nx'])

grouped = da.groupby('nx')

for label, group in grouped:
    print(group)
#+END_SRC
#+CAPTION:
[[file:$1]]

** TAO daily dataset
** TAO hourly dataset
#+BEGIN_SRC jupyter-python :file images/tao-marginal-stability-hourly.png
adcp = pump.obs.read_tao_adcp(freq='hr')
temp = pump.obs.read_eq_tao_temp_hr()
Ri = pump.calc_tao_ri(adcp, temp)

eucmax = pump.get_euc_max(adcp.u)
Ri = Ri.to_dataset()
Ri['zeuc'] = Ri.depth - eucmax

seasonal = Ri.groupby('time.season')

for season, Ris in seasonal:
    Rigrouped = Ris.Ri.groupby_bins(Ris.zeuc, np.arange(0, 200, 10))
    for bin, group in Rigrouped:
        print(bin)

seasonal = (Ri
            .groupby('time.season').median('time')
            .reindex(season=['DJF', 'MAM', 'JJA', 'SON']))

fg = (seasonal.plot.line(col='longitude', hue='season', y='depth',
                         ylim=[-150, 0], xlim=[0.1, 3.5], xscale='log'))
fg.map(lambda: dcpy.plots.linex([0.25, 0.3]))
plt.gcf().suptitle('Seasonal median 5m Ri | Hourly mean TAO ADCP, T '
                   , y=1.02)
plt.gcf().set_size_inches((8, 4))
plt.gcf().set_dpi(200)
# f, ax = plt.subplots(1, 1, constrained_layout=True)
# f.savefig('images/tao-marginal-stability-hourly.png')
#+END_SRC

#+RESULTS:
[[file:images/tao-marginal-stability-hourly.png]]


Check Ri
#+BEGIN_SRC jupyter-python
V = adcp[['u', 'v']]
S2 = (V['u'].differentiate('depth')**2
      + V['v'].differentiate('depth')**2)

T = (temp
     .sel(time=V.time)
     .sortby('depth')
     .interpolate_na('depth', 'linear')
     .sortby('depth', 'descending')
     .interp(depth=V.depth))

# the calculation is sensitive to using sw.alpha! can't just do 1.7e-4
N2 = (9.81
      ,* dcpy.eos.alpha(35, T, T.depth)
      ,* T.differentiate('depth'))

N2 = N2
Ri = N2.where(N2 > 1e-7) / S2.where(S2 > 1e-10)
#+END_SRC
#+CAPTION:
[[file:images/temp/imgcsSb04.png]]

*** EUC relative depth coordinate
No luck yet.

#+BEGIN_SRC jupyter-python

def split_by_chunks(obj):
    import itertools
    chunk_slices = {}

    if isinstance(obj, xr.DataArray):
        dataset = obj._to_temp_dataset()
    else:
        dataset = obj
    for dim, chunks in dataset.chunks.items():
        slices = []
        start = 0
        for chunk in chunks:
            stop = start + chunk
            slices.append(slice(start, stop))
            start = stop
        chunk_slices[dim] = slices
    for slices in itertools.product(*chunk_slices.values()):
         selection = dict(zip(chunk_slices.keys(), slices))
         yield (selection, dataset[selection])

def reconstruct_from_chunks(template, chunks):
    dsnew = xr.zeros_like(Ri.to_array())
    for (selection, subset) in chunks:
        dsnew.loc[selection] = subset
    return dsnew


chunks = [cc for cc in split_by_chunks(Ri.chunk({'time': 10000}))]
Rinew = reconstruct_from_chunks(Ri, chunks)
xr.testing.assert_equal(Ri, Rinew)


import scipy as sp
Ri['zeuc'] = Ri.zeuc.transpose(*Ri.Ri.dims)
subset = Ri.isel(time=slice(8000, 2*8000), longitude=2)

tmat = xr.broadcast(subset.zeuc, subset.time)[1].values
Ri_binned = sp.stats.binned_statistic_2d(tmat,
                                         subset.zeuc.values,
                                         subset.Ri.values,
                                         statistic='mean',
                                         bins=np.arange(0, 200, 5))

#+END_SRC

** Simple models for MI

1. I am averaging daily TAO mooring data over all time. Is this a good idea?

#+NAME: estimate-Ri-diagnosis-terms
#+BEGIN_SRC jupyter-python :results none
def estimate_euc_depth_terms(ds):

    ds.load()
    ds['us'] = ds.u.isel(depth=0)
    ds['ueuc'] = ds.u.sel(depth=ds.eucmax, longitude=ds.longitude, method='nearest')
    ds['du'] = ds.us - ds.ueuc
    ds.du.attrs['long_name'] = '$\Delta$u'

    if 'dens' in ds:
        ds['b'] = (ds.dens-1025) * -9.81/1025
        ds['bs'] = ds.b.isel(depth=0)
        ds['beuc'] = ds.b.sel(depth=ds.eucmax, longitude=ds.longitude, method='nearest')
        ds['db'] = ds.bs - ds.beuc
        ds.db.attrs['long_name'] = '$\Delta$b'

    return ds

if 'gcm1' in locals():
    print('skipping gcm1, jra, ssh')
    subset = (gcm1.annual.sel(latitude=0, method='nearest')
              .assign_coords(latitude=0)
              .squeeze()
              .sel(depth=slice(0, -250)))
    subset['dens'] = pump.mdjwf.dens(subset.salt, subset.theta, subset.depth)
    subset['eucmax'] = pump.calc.get_euc_max(subset.u)

    subset = estimate_euc_depth_terms(subset)

    jra = (pump.obs.read_jra()
           .sel(latitude=0, method='nearest')
           .sel(time='1996')
           .load())
    jra['tau'] = jra.Uwind.copy(
        data=airsea.windstress.stress(np.hypot(jra.Uwind, jra.Vwind)))

    mean_jra = jra.mean('time')
    ssh = xr.open_mfdataset(pump.obs.root + 'make_TPOS_MITgcm/1996/SSH*.nc').zos

johnson = (pump.obs.read_johnson()
           .sel(latitude=0)
           .rename({'rho': 'dens'}))
johnson['eucmax'] = pump.get_euc_max(johnson.u)
johnson = estimate_euc_depth_terms(johnson)
johnson.attrs['name'] = 'Johnson'

# need to fill to the surface
tao_adcp = pump.obs.read_tao_adcp().mean('time').bfill('depth')
tao_adcp['eucmax'] = pump.get_euc_max(tao_adcp.u)
tao_adcp = estimate_euc_depth_terms(tao_adcp)

tao_ctd = (pump.obs.read_tao()
           .sel(latitude=0, longitude=tao_adcp.longitude)
           .mean('time')
           .compute())
tao_ctd['eucmax'] = tao_adcp.eucmax
tao_ctd['dens'] = pump.mdjwf.dens(np.array(35.0), tao_ctd.temp, tao_ctd.depth)

tao_ctd = estimate_euc_depth_terms(tao_ctd
                                   .sortby('depth')
                                   .interpolate_na('depth')
                                   .sortby('depth', ascending=False)
                                   .bfill('depth'))

tao.attrs['name'] = 'TAO'
tao = xr.merge([tao_adcp[['us', 'ueuc', 'du', 'eucmax']],
                tao_ctd[['bs', 'beuc', 'db']]])
#+END_SRC

*** Simple Ri calculation

Another way to do this is to think of Ri=0.5 or something far east (e..g. 195W in the TAO image). Given crude estimates of longitudinal changes in Îb, EUCmax depth etc. can we explain the drop in Ri to 0.25 by 170W?

#+CAPTION: Ri estimated using hourly TAO data.
[[file:images/tao-marginal-stability-hourly.png]]

Use a bulk definition of Richardson number
#+begin_export latex
\begin{align}
Ri &= \frac{Îb h}{ÎuÂ²} \\
\log Ri &= \log Îb + \log h - 2 \log Îu \\
\frac{1}{Ri} â_x Ri = \frac 1h â_xh + \frac{1}{Îb} â_x Îb - 2 \frac{1}{Îu} â_x Îu
\end{align}
#+end_export

Let RHS =  Î±,
#+begin_export latex
\begin{align}
\pp{Ri}{x} &= \Ri Î± \\
\Ri_0 + \pp{Ri}{x} Îx &= 0.25 \\
\Ri_0 + \Ri_0 Î± Îx &= 0.25 \\
Î± &= \frac{(0.25/\Ri_0 - 1)}{Îx} \\
\end{align}
#+end_export

$\Ri_0$ is $\Ri$ at 195W = 0.5, Îx = (170W-195W) = 25Â° â Î± = -1/50Â° approx.

1. Johnson dataset don't show marginal stability at 0.25. So the estimates of Îu, Îb are probably wrong. But it looks like this dataset represents marginal stability at /bulk/ Ri â 1.
2. And there is a big change between 200W and 140W
3. This big change is largely from Îu

#+call: estimate-Ri-diagnosis-terms()
#+NAME: Ri-diagnosis-johnson
#+BEGIN_SRC jupyter-python :file images/Ri-diagnosis-johnson.png
with xr.set_options(keep_attrs=False):
    johnson['h'] = (johnson.eucmax)
    johnson['h'].attrs['long_name'] = 'h'
    johnson['Ri'] = johnson.db * np.abs(johnson.h) / (johnson.du**2)

    tao['h'] = tao.eucmax
    tao['h'].attrs['long_name'] = 'h'
    tao['Ri'] = tao.db * np.abs(tao.h) / (tao.du ** 2)


def plot_bulk_Ri_diagnosis(ds, **kwargs):

    def plot_ri_contrib(ax1, ax2, v, Ri, factor=1, **kwargs):
        #v2 = (v.copy(deep=True)
        #      .rolling(longitude=3, min_periods=1, center=True)
        #      .mean())
        v2 = v
        dvdx = v2.differentiate('longitude')
        # dvdx = v.diff('longitude') / v.longitude.diff('longitude')
        # dvdx = dvdx.reindex(longitude=v.longitude)
        per = factor * dvdx / v2
        hl = per.plot(ax=ax2, x='longitude',
                      label=f'{factor}/{v.name} $â_x${v.name}',
                      ,**kwargs)
        v2.plot(ax=ax1, x='longitude', color=hl[0].get_color(), **kwargs)
        ax1.set_xlabel('')
        ax1.set_title('')

        return per

    f, axx = plt.subplots(7, 1, constrained_layout=True, sharex=True,
                          gridspec_kw={'height_ratios': [1, 1, 1, 1, 1, 1, 2]})
    ax = dict(zip(['Ri', 'h', 'du', 'db', 'u', 'b'], axx[:-1]))

    factor = dict(zip(ax.keys(), [1, 1, -2, 1]))
    rhs = xr.zeros_like(ds.h)
    for var in ax.keys():
        if var not in ['u', 'b']:
            per = plot_ri_contrib(ax[var], axx[-1], ds[var], ds.Ri, factor[var])
        if var != 'Ri':
            rhs += per

    for vv in ['u', 'b']:
        for vvar in ['s', 'euc']:
            var = vv + vvar
            ds[var].differentiate('longitude').plot(ax=ax[vv],
                                                    label=f'$â_x{vv}_{{{vvar}}}$',
                                                    ,**kwargs)
            ax[vv].legend()
            ax[vv].set_title('')
            ax[vv].set_xlabel('')
            ax[vv].set_ylabel('')
            dcpy.plots.liney(0, ax[vv])

    ax['u'].set_ylim([-0.02, 0.02])
    ax['b'].set_ylim([-0.0005, 0.0005])

    rhs.plot(ax=axx[-1], x='longitude', color='k', label='RHS')
    ax['Ri'].set_yscale('log')
    ax['Ri'].set_yticks([0.25, 0.5, 1, 5, 10]);
    ax['Ri'].grid(True)

    axx[-1].set_ylabel('Fractional changes')
    axx[-1].legend()
    dcpy.plots.liney(0, ax=axx[-1])

    name = ds.attrs['name']
    axx[0].set_title(f"latitude = 0, {name} dataset")
    f.set_size_inches(8, 10)

plot_bulk_Ri_diagnosis(johnson)
#+END_SRC

#+RESULTS: Ri-diagnosis-johnson
[[file:images/Ri-diagnosis-johnson.png]]

With the Johnson data, it looks like the longitudinal variation in Îu is what dominates the reduction in Ri. This seems to be mostly due to an accelerating EUC but also due to a reversal in sign of surface current starting at 195W.

#+BEGIN_SRC jupyter-python :file images/johnson-eq-section.png
f, ax = plt.subplots(2, 1, constrained_layout=True)
plt.sca(ax[0])
johnson.u.plot()
johnson.u.plot.contour(levels=10, colors='k', linewidths=1)
johnson.h.plot(color='w', linewidth=2, linestyle='--')

plt.sca(ax[1])
(johnson.b-johnson.beuc).plot(cbar_kwargs={'label': '$b - b_{euc}$'})
johnson.u.plot.contour(levels=10, colors='k', linewidths=1)
johnson.h.plot(color='w', linewidth=2, linestyle='--')
plt.gcf().set_size_inches(8, 8)

ax[0].set_title('Johnson mean')
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1.0, 'Johnson mean')
[[file:images/johnson-eq-section.png]]
:END:

#+BEGIN_SRC jupyter-python
v = ds.Ri;
v2 = v.rolling(longitude=3,center=True, min_periods=1).mean()

f, ax = plt.subplots(2, 1)
v.plot(ax=ax[0])
v2.plot(ax=ax[0])

(v.differentiate('longitude')).plot(ax=ax[1])
(v2.differentiate('longitude')).plot(ax=ax[1])
#+END_SRC

#+RESULTS:
:RESULTS:
| <matplotlib.lines.Line2D | at | 0x2b9e2852b630> |
[[file:./.ob-jupyter/b33f66676dbabe44345a64e27aa3bd23edb28892.png]]
:END:


Similar result holds for TAO though errors are larger. Why is that?

#+BEGIN_SRC jupyter-python :file Ri-diagnosis-tao.png
plot_bulk_Ri_diagnosis(tao)
#+END_SRC

#+RESULTS:
[[file:Ri-diagnosis-tao.png]]

*** Simple model

#+begin_export latex
\Ri = \frac{\bar{w} h Q}{\left(g(hÎ·_x + Îb/2 h_x) + Ï_w \right)Â²}
#+end_export

**** term magnitudes

We haven't saved SSH!

#+CALL: estimate-terms()
#+BEGIN_SRC jupyter-python :file images/eq-simple-model-terms.png
def plot_eucmax(ax):
    heuc = eucmax.plot(ax=ax, x='longitude', color='k', lw=1, _labels=False)
    dcpy.plots.annotate_end(heuc[0], 'eucmax')

def plot_line(ax, da, label):
    hu = da.plot(ax=ax, x='longitude')
    dcpy.plots.annotate_end(hu[0], label)

f, axx = plt.subplots(4, 2, sharex=True, constrained_layout=True)
ax = dict(zip(['u', 'b', 'du', 'db', 'h', 'ssh', 'Q', 'tau'], axx.flat))
# ax['Q'] = ax['tau'].twinx()

label_kwargs = dict(fmt='%.1f', colors='k', fontsize='smaller')

hu = subset.u.plot.contourf(levels=11, ax=ax['u'], y='depth',
                            cbar_kwargs={'orientation': 'horizontal'})
#ax['u'].clabel(hu, **label_kwargs)

hb = (subset.b).plot.contourf(levels=11, ax=ax['b'], y='depth',
                              cbar_kwargs={'label': 'b+9.81',
                                           'orientation': 'horizontal'})
# ax['b'].clabel(hb, **label_kwargs)
heuc = subset.eucmax.plot(ax=ax['b'], x='longitude', color='k', lw=1, _labels=False)
dcpy.plots.annotate_end(heuc[0], 'eucmax')

[plot_eucmax(aa) for aa in [ax['u'], ax['b']]]

[plot_line(ax['du'], da, label)
 for (da, label) in zip([us, ueuc, du],
                        ['$u_{surf}$', '$u_{euc}$', '$\Delta u$'])]

[plot_line(ax['db'], da, label)
 for (da, label) in zip([bs, beuc, db],
                        ['$b_{surf}$', '$b_{euc}$', '$\Delta b$'])]

dcpy.plots.liney(0, ax=[ax['du'], ax['db']])

def mark_median(ax, hxmed):
    hxmed = dhdx.median()
    dcpy.plots.liney(hxmed, ax=ax)
    ax.set_yticks(ax.get_yticks() + [hxmed])

eucmax.plot(ax=ax['h'])
johnson.eucmax.plot(ax=ax['h'])
adcp.eucmax.plot(ax=ax['h'], marker='o')
ax['h'].set_ylabel('$h$')

ax['hx'] = ax['h'].twinx()
dhdx = (eucmax.rolling(longitude=100).mean()
        .differentiate('longitude') / 110e3)[10:-10]
(dhdx.plot(ax=ax['hx'], x='longitude'))
mark_median(ax['hx'], dhdx.median())

dhdx = (johnson.eucmax.differentiate('longitude')/110e3)
(dhdx.plot(ax=ax['hx'], x='longitude'))
mark_median(ax['hx'], dhdx.median())

dhdx = (adcp.eucmax.differentiate('longitude')/110e3)
(dhdx.plot(ax=ax['hx'], x='longitude', marker='o'))
mark_median(ax['hx'], dhdx.median())

ax['hx'].set_ylabel('$h_x$')

mean_ssh = (ssh.sel(latitude=0).mean('time')).load()
mean_ssh.attrs['long_name'] = 'ssh'
mean_ssh.plot(ax=ax['ssh'])

ax['sshx'] = ax['ssh'].twinx()
dsshdx = (mean_ssh.rolling(longitude=20).mean()
          .differentiate('longitude') / 110e3)[10:-10]
(dsshdx.plot(ax=ax['sshx'], x='longitude', _labels=False))
sshxmed = dsshdx.median()
dcpy.plots.liney(sshxmed, ax=ax['sshx'])
ax['sshx'].set_yticks(ax['sshx'].get_yticks() + [sshxmed])
ax['sshx'].set_ylabel('ssh$_x$')

subset.oceQnet.plot(ax=ax['Q'])

mean_jra.tau.plot(ax=ax['tau'], x='longitude')
[aa.set_title('') for aa in ax.values()]

axx[0,0].set_xlim([-230, -95])
f.suptitle('latitude=0, 1996 annual mean')
f.set_size_inches((10, 8))
#+END_SRC

 #+RESULTS:
 [[file:images/eq-simple-model-terms.png]]

#+BEGIN_SRC jupyter-python
limits=dict(vmin=-5e-7, vmax=5e-7, cmap=mpl.cm.RdBu_r, ylim=[-250, 0])

f, ax = plt.subplots(3, 1, constrained_layout=True, sharex=True, sharey=True)
(subset.u.differentiate('longitude')/110e3).plot(
    ax=ax[0], **limits, add_colorbar=False)

(gcm1.annual.v.differentiate('latitude')/110e3).sel(latitude=0, method='nearest').plot(
    ax=ax[1], **limits, add_colorbar=False)

(-1*subset.w.differentiate('depth')).plot(
    ax=ax[2], **limits, cbar_kwargs=dict(orientation='horizontal'))

[plot_eucmax(aa) for aa in ax]
[aa.set_title('') for aa in ax[1:]]
[aa.set_xlabel('') for aa in ax[:-1]]
#+END_SRC
#+RESULTS:
:RESULTS:
| Text | (0.5 0 ) | Text | (0.5 0 ) |
[[file:./.ob-jupyter/d24f4d57382966c72d47eadb7d817edffa521c5d.png]]
:END:

#+BEGIN_SRC jupyter-python
johnson.u.plot(y='depth')
johnson.eucmax.plot(color='k')
#+END_SRC

#+RESULTS:
:RESULTS:
| <matplotlib.lines.Line2D | at | 0x2af69e8b15c0> |
[[file:./.ob-jupyter/41d85dc5e7e3f620c92e7a0e32dfb1d2cdc839ae.png]]
:END:

* Upper Core Layer

- seems to be approx constant $u_z$
-

** An example

#+NAME: extract-ucl
#+BEGIN_SRC jupyter-python :results none
if 'gcm1' not in locals():
    gcm1 = pump.model('/glade/p/nsc/ncgd0043/TPOS_MITgcm_1_hb/HOLD/',
                      name='gcm1', full=True)

extract = (gcm1.full.sel(time=slice('1995-11-15', '1995-12-10'),
                         depth=slice(0, -200),
                         latitude=slice(-3, 3),
                         longitude=slice(-150, -130)))

extract.load()
#+END_SRC

#+BEGIN_SRC jupyter-python :file images/ucl-example.png :results none
%matplotlib inline

# extract = subset.where(subset.period == 5, drop=True).sel(depth=slice(-40, -150))
region = dict(time='1995-11-22 00:00', longitude=-140, method='nearest')

f, ax = plt.subplots(1, 3, sharex=True, sharey=True, constrained_layout=True)

for aa, vv in zip(ax, ['theta', 'v', 'u']):
    ((extract.salt
      .sel(**region))
      .plot(ax=aa, y='depth', cmap=mpl.cm.RdYlBu_r, robust=True,
            cbar_kwargs={'orientation': 'horizontal', 'aspect': 20}))

    (extract[vv].sel(**region)
     .plot.contour(ax=aa, levels=22, add_labels=False, y='depth', colors='k', linewidths=0.4))

    title = aa.get_title()
    aa.set_title(f'salt [color] & {vv} [contours]')
    aa.set_ylim([-180, 0])

f.suptitle(title, y=1.05)
f.set_size_inches((10, 5))
#+END_SRC
#+CAPTION:
[[file:imags/ucl-example.png]]

** PV calculation

#+BEGIN_SRC jupyter-python
# ds = gcm1.full
def pv(ds):
    ds['b'] = ds.dens * -9.81/1025
    ds['b'].attrs['long_name'] = '$b$'
    ds['b'].attrs['description'] = 'buoyancy'

    f = 2*(np.pi/86400)  * np.sin(ds.latitude * np.pi/180)
    zeta = ds.v.differentiate('longitude') - ds.u.differentiate('latitude')
    q = ((f + zeta) * ds.b.differentiate('depth')
         - ds.v.differentiate('depth') * ds.b.differentiate('longitude')
         + ds.u.differentiate('depth') * ds.b.differentiate('latitude'))

    return q

# f, ax = plt.subplots(1, 1, constrained_layout=True)
#+END_SRC
#+CAPTION:
[[file:$1]]

* TIW compositing
* MLD calculation

#+BEGIN_SRC jupyter-python :results none
gcm5 = pump.model('../glade/TPOS_MITgcm_5/HOLD/',
                  name='gcm5', full=True, budget=False)
gcm5.full = gcm5.full.chunk({'depth': 68, 'latitude': 240, 'longitude': 500}) # 12MB chunks
#+END_SRC

#+BEGIN_SRC jupyter-python
old_index = gcm5.full.indexes['time']

new_index = old_index.copy()
new_index.freq = pd.tseries.frequencies.to_offset(
    pd.infer_freq(gcm5.full.indexes['time']))

gcm5.full = gcm5.full.reindex(time=new_index)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python
gcm25 = pump.model('../glade/TPOS_MITgcm_2.5/HOLD/',
                   name='gcm2.5', full=True, budget=False)
gcm25.full = (gcm25.full
              .chunk({'depth': 100, 'latitude': 120, 'longitude': 500})) # 12MB chunks
#+END_SRC

#+BEGIN_SRC jupyter-python
import pump.mdjwf
# gcm25.full = gcm25.full.chunk({'depth': 100, 'latitude': 240, 'longitude': 500}) # 12MB chunks
model = gcm5
subset = model.full.sel(depth=slice(0, -240))
# dens = dcpy.eos.dens(subset.salt, subset.theta, subset.depth)
dens = pump.mdjwf.dens(subset.salt, subset.theta, subset.depth)
mld = pump.get_mld(dens)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :file images/temp/mld-test.png
%matplotlib inline

f, ax = plt.subplots(1, 1, constrained_layout=True)
region = dict(latitude=0, longitude=-140, method='nearest')
itime = 100
subset2 = subset.isel(time=itime).sel(**region)
dens.isel(time=itime).sel(**region).plot(ax=ax, y='depth')
dcpy.plots.liney(mld.isel(time=itime).sel(**region))
# f.savefig('..//images/mld-test.png')
#+END_SRC

#+RESULTS:
[[file:images/temp/mld-test.png]]

* EQUIX analysis

#+NAME: read-equix
#+BEGIN_SRC jupyter-python :session py :results none
import dcpy.util
import dcpy.oceans
import numpy as np
import xarray as xr

from scipy.io import loadmat

import platform

if platform.uname().node == 'darya':
    dirname = 'obs/equix/'
else:
    dirname = 'glade/obs/equix/'


adcpmat = loadmat(dirname + '03UP_10min_mag_corrected.mat')

adcp = xr.Dataset()
adcp['depth'] = xr.DataArray(adcpmat['Zgrid'].squeeze(),
                             dims=['depth'])
adcp['time'] = xr.DataArray(
    dcpy.util.datenum2datetime(adcpmat['jday_gmt'].squeeze()),
    dims=['time'],
    attrs={'timezone': 'GMT'})

adcp['u'] = (('depth', 'time'), adcpmat['Ug'])
adcp['v'] = (('depth', 'time'), adcpmat['Vg'])
adcp['w'] = (('depth', 'time'), adcpmat['Wg'])
adcp.attrs['declination'] = adcpmat['magdeclination'].squeeze()

ctdmat = loadmat(dirname + 'sbe37_eq08_10min.mat')
ctd = xr.Dataset()
ctd['depth'] = (('depth'), ctdmat['zgrid'].squeeze())
ctd['time'] = (('time'),
               dcpy.util.datenum2datetime(
                   ctdmat['Jday_gmt'].squeeze()))
ctd['T'] = (('depth', 'time'), ctdmat['T_tgrid_zgrid'])
ctd['S'] = (('depth', 'time'), np.real(ctdmat['S_tgrid_zgrid']))
ctd['C'] = (('depth', 'time'), ctdmat['C_tgrid_zgrid'])
ctd['pden'] = (('depth', 'time'), ctdmat['pden_tgrid_zgrid'])
ctd['pden'] = np.real(ctd.pden)

ctd = ctd.sel(time=slice('2008-10-24 06:30', '2008-11-04 17:50'))
adcp = adcp.sel(time=slice('2008-10-24 06:30', '2008-11-04 17:50'))

ctd['time'] = ctd.time.dt.round('min')
adcp['time'] = adcp.time.dt.round('min')

ctd['depth'] = ctd.depth * -1
adcp = adcp.sortby('depth', ascending=False)
ctd = ctd.sortby('depth', ascending=False)

adcp['shear'] = np.hypot(adcp.u, adcp.v)
import pump
adcp['t90'] = pd.to_datetime('2008-Oct-29 22:15')
xr.testing.assert_equal(ctd.time, adcp.time)
#+END_SRC

#+CALL: read-equix()
#+BEGIN_SRC jupyter-python :session py
def plot_2dspectrum(da, ax=None, dim=None, linthreshx=0.1, linscalex=0.05,
                    linthreshy=1e-3, linscaley=0.01, diff=None, **kwargs):

    from xrft import xrft

    if ax is None:
        ax = plt.gca()

    spec = xrft.power_spectrum(da, dim=dim, detrend='constant', window=True,
                               density=True)

    if diff:
        spec = (2*np.pi * spec['freq_' + dim[0]])**2 * spec
        spec = spec.where(spec > 0)

    spec.plot(norm=mpl.colors.LogNorm(), ax=ax, robust=True, **kwargs)

    # ax.set_yscale('symlog', linthreshy=linthreshy, linscaley=linscaley)
    # ax.set_xscale('symlog', linthreshx=linthreshx, linscalex=linscalex)


f, ax = plt.subplots(1, 1, constrained_layout=True)

plot_2dspectrum(adcp.u.sel(depth=slice(-50, -20)), dim=['depth'], diff=True)
f.savefig('images/temp/imgHqJpjd.png')
#+END_SRC

#+RESULTS:

#+CAPTION:
[[file:images/temp/imgHqJpjd.png]]


#+BEGIN_SRC jupyter-python :session py
adcp = adcp.dropna('depth', how='any')
adcp['shear'] = adcp.u.differentiate('depth') + 1j * adcp.v.differentiate('depth')
spec = xrft.power_spectrum(adcp.shear, dim=['depth'], density=True, detrend='linear', window=True)

plt.figure)(git
(spec.coarsen(dict(freq_time=5, freq_depth=4), boundary='trim')
 .mean().plot(norm=mpl.colors.LogNorm(), robust=True, cmap=mpl.cm.Reds))
# f, ax = plt.subplots(1, 1, constrained_layout=True)

# f.savefig('images/temp/imgeoNH8r.png')
#+END_SRC
#+CAPTION:
[[file:images/temp/imgeoNH8r.png]]

* read POP

#+BEGIN_SRC jupyter-python
pth = '/glade/scratch/altuntas/archive/g.e20.G.TL319_t13.control.001_hfreq/ocn/hist/mavg/'
fls = 'g.e20.G.TL319_t13.control.001_hfreq.pop.h.00'

years = range(33,53)
offset = 1957
months = [str(xx).zfill(2) for xx in range(1,13,1)]

files = []
for y in years:
    for m in months:
        files.append(pth + fls + str(y) + '-' + m + '.nc')
#+END_SRC

#+BEGIN_SRC jupyter-python
def read_pop(files):
    def preprocess(ds):
        return ds[['VVEL', 'TEMP']].reset_coords(drop=True)

    ds = xr.open_mfdataset(files, parallel=True, preprocess=preprocess)
    file0 = xr.open_dataset(files[0])
    ds.update(file0[['TLONG', 'TLAT', 'ULONG', 'ULAT']])
    file0.close()

    return ds
#+END_SRC

* johnson
#+BEGIN_SRC jupyter-python :session py
import pump

johnson = pump.obs.read_johnson('~/datasets/johnson-eq-pac-mean-adcp.cdf')
johnson['b'] = (-9.81/1025) * johnson.rho

lat = 0
f, ax = plt.subplots(2, 1, constrained_layout=True)

johnson.u.sel(latitude=lat).plot.contourf(ax=ax[0], cmap=mpl.cm.RdBu_r, levels=20)
hc = johnson.rho.sel(latitude=lat).plot.contour(ax=ax[0], colors='k', levels=11)
ax[0].clabel(hc, fmt='%.1f')

N2 = johnson.b.differentiate('depth')
N2.attrs['long_name'] = '$N^2$'
N2.sel(latitude=lat).plot.contourf(ax=ax[1], cmap=mpl.cm.Blues, levels=20, vmin=0)
johnson.u.sel(latitude=lat).plot.contour(ax=ax[1], levels=10, colors='k')

f, ax = plt.subplots(3, 1, constrained_layout=True)
(johnson.u
 .sel(latitude=0)
 .differentiate('depth')
 .plot.contourf(ax=ax[0], robust=True, levels=12,
                cbar_kwargs=dict(label='$u_z$')))

(johnson.b
 .differentiate('latitude')
 .sel(latitude=0)
 .plot.contourf(ax=ax[1], robust=True, levels=12, cbar_kwargs=dict(label='$b_y$')))

(johnson.b
 .differentiate('longitude')
 .sel(latitude=0)
 .plot.contourf(ax=ax[2], robust=True, levels=12, cbar_kwargs=dict(label='$b_x$')))
#+END_SRC
